{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from scipy.spatial import distance\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import hdpmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Month_days = {1 : 31, 2 : 29, 3 : 31, 4 : 30, 5 : 31, 6 : 30,\n",
    "             7 : 31, 8 : 31, 9 : 30, 10 : 31, 11 : 30, 12 : 31}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(start_month = 1, end_month = None, date_specified = False, start_date = 1, end_date = 31):\n",
    "    articles = []\n",
    "    for l in range(start_month,end_month+1):\n",
    "        if(date_specified is False):\n",
    "            end_date = Month_days[l]\n",
    "        for i in range(start_date, end_date+1):\n",
    "            dataP = pd.read_csv(str(l) + \"_\" + str(i) + \"_P.csv\")\n",
    "            dataTH = pd.read_csv(str(l) + \"_\" + str(i) + \"_TH.csv\")\n",
    "            dataTOI = pd.read_csv(str(l) + \"_\" + str(i) + \"_TOI.csv\")\n",
    "            for j in range(len(dataP)):\n",
    "                articles.append(dataP['Article'][j])\n",
    "            for j in range(len(dataTH)):\n",
    "                articles.append(dataTH['Content'][j])\n",
    "            for j in range(len(dataTOI)):\n",
    "                articles.append(dataTOI['Content'][j]) \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(articles):\n",
    "    temp = []\n",
    "    for article in articles:\n",
    "        if type(article) is str:\n",
    "            temp.append(article.lower().translate(str.maketrans('','' , string.punctuation)))\n",
    "    for i in range(len(temp)):\n",
    "        temp[i] = \" \".join(temp[i].split())\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_preprocessing(articles):\n",
    "    words = []\n",
    "    for article in articles:\n",
    "        words.append(word_tokenize(article))\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        words[i] = [word for word in words[i] if word not in stop_words]  \n",
    "\n",
    "    for i in range(len(words)):\n",
    "        words[i] = [token for token in words[i] if not token.isnumeric()] \n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(words)):\n",
    "        words[i] = [lemmatizer.lemmatize(j) for j in words[i]]\n",
    "\n",
    "    for i in range(len(words)): \n",
    "        words[i] = [token for token in words[i] if len(token) > 1]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_articles(1, 3)\n",
    "articles = standardize_data(articles)\n",
    "words = word_preprocessing(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_preprocessing(words):\n",
    "    bigram = Phrases(words, min_count=5,threshold= 100)\n",
    "    for idx in range(len(words)):\n",
    "        for token in bigram[words[idx]]:\n",
    "            if '_' in token:\n",
    "                words[idx].append(token)    \n",
    "    dictionary = Dictionary(words)\n",
    "    dictionary.filter_extremes(no_below = 5000 ,no_above = 0.3, keep_n = 100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in words]\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    return dictionary, corpus_tfidf, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, corpus_tfidf, words = LDA_preprocessing(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 6\n",
    "chunksize = 5000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, chunksize=chunksize, \n",
    "                 alpha='auto', eta='auto', iterations=iterations, num_topics=num_topics,\n",
    "                 passes=passes, eval_every=eval_every )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = datapath(\"model2\")\n",
    "model.save((temp_file))\n",
    "\n",
    "model2 = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(model, corpus_tfidf, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = model.get_document_topics(corpus_tfidf, per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print('New Document \\n')\n",
    "    print('Document topics:', all_topics[i])\n",
    "    print(\" \")\n",
    "    print('-------------- \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distance.jensenshannon(doc1,doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_file)\n",
    "#model_mar = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import hdpmodel\n",
    "hdp_model = hdpmodel.HdpModel(corpus=corpus_tfidf, id2word=id2word)\n",
    "hdp_model.print_topics(num_topics=-1)\n",
    "\n",
    "shown_topics = hdp_model.show_topics(num_topics=hdp_model.m_T, formatted=False)\n",
    "topics_nos = [x[0] for x in shown_topics]\n",
    "weights = [ sum([item[1] for item in shown_topics[topicN][1]]) for topicN in topics_nos ]\n",
    "ll= pd.DataFrame({'topic_id' : topics_nos, 'weight' : weights})\n",
    "\n",
    "for i in range(0,150):\n",
    "    print(ll['weight'][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(words)):\n",
    "    top_topics = model.get_document_topics(corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(6)]\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles = get_articles(4,4,date_specified = True, start_date = 1, end_date = 1)\n",
    "\n",
    "test_articles = standardize_data(test_articles)\n",
    "test_words = word_preprocessing(test_articles)\n",
    "\n",
    "testcorpus = [dictionary.doc2bow(doc) for doc in test_words]\n",
    "\n",
    "test_vecs = []\n",
    "for i in range(len(test)):\n",
    "    top_topics = model.get_document_topics(testcorpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(6)]\n",
    "    test_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "topic2 = []\n",
    "topic3 = []\n",
    "topic4 = []\n",
    "topic5 = []\n",
    "matched_topic = [topic0, topic1, topic2, topic3, topic4, topic5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_vecs)):\n",
    "    mx = -1\n",
    "    for j in range(len(test_vecs[i])):\n",
    "        mx = max(mx, test_vecs[i][j])\n",
    "    \n",
    "    for j in range(len(test_vecs[i])):\n",
    "        if mx==test_vecs[i][j]:\n",
    "            matched_topic[j].append(test_articles[i])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((matched_topic[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_tag(tag):\n",
    "    \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "\n",
    "    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "    try:\n",
    "        return tag_dict[tag[0]]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def doc_to_synsets(doc):\n",
    "    \"\"\"\n",
    "    Returns a list of synsets in document.\n",
    "\n",
    "    Tokenizes and tags the words in the document doc.\n",
    "    Then finds the first synset for each word/tag combination.\n",
    "    If a synset is not found for that combination it is skipped.\n",
    "\n",
    "    Args:\n",
    "        doc: string to be converted\n",
    "\n",
    "    Returns:\n",
    "        list of synsets\n",
    "\n",
    "    Example:\n",
    "        doc_to_synsets('Fish are nvqjp friends.')\n",
    "        Out: [Synset('fish.n.01'), Synset('be.v.01'), Synset('friend.n.01')]\n",
    "    \"\"\"\n",
    "\n",
    "    # Your Code Here\n",
    "    token = nltk.word_tokenize(doc)\n",
    "    # add parts of speech to token\n",
    "    tag = nltk.pos_tag(token)\n",
    "    # convert nltk pos into wordnet pos\n",
    "    nltk2wordnet = [(i[0], convert_tag(i[1])) for i in tag]\n",
    "    # if there are no synsets in token, ignore, else put in a list\n",
    "    output = [wn.synsets(i, z)[0] for i, z in nltk2wordnet if len(wn.synsets(i, z))>0]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def similarity_score(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculate the normalized similarity score of s1 onto s2\n",
    "\n",
    "    For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "    Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "    number of largest similarity values found.\n",
    "\n",
    "    Args:\n",
    "        s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "    Returns:\n",
    "        normalized similarity score of s1 onto s2\n",
    "\n",
    "    Example:\n",
    "        synsets1 = doc_to_synsets('I like cats')\n",
    "        synsets2 = doc_to_synsets('I like dogs')\n",
    "        similarity_score(synsets1, synsets2)\n",
    "        Out: 0.73333333333333339\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Your Code Here\n",
    "    list1 = []\n",
    "    # For each synset in s1\n",
    "    for a in s1:\n",
    "        # finds the synset in s2 with the largest similarity value\n",
    "        list1.append(max([i.path_similarity(a) for i in s2 if i.path_similarity(a) is not None]))\n",
    "\n",
    "    output = sum(list1)/len(list1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def document_path_similarity(doc1, doc2):\n",
    "    \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
    "            # first function u need to create\n",
    "    synsets1 = doc_to_synsets(doc1)\n",
    "    synsets2 = doc_to_synsets(doc2)\n",
    "            # 2nd function u need to create\n",
    "    return (similarity_score(synsets1, synsets2) + similarity_score(synsets2, synsets1)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = np.zeros((len(matched_topic[0]),len(matched_topic[0])))\n",
    "t             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(matched_topic[0])):\n",
    "              doc1 = test_articles[matched_topic[0][i]]\n",
    "              for j in range(i, len(matched_topic[0])):\n",
    "                  doc2 = test_articles[matched_topic[0][j]]\n",
    "                  if i == j:\n",
    "                      t[i][j]= -1\n",
    "                      continue\n",
    "                  t[i][j] = document_path_similarity(doc1, doc2)\n",
    "                  t[j][i] = t[i][j]\n",
    "                  print(t[i][j], end= '')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "count_vectorizer = TfidfVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(matched_topic[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "a=[]\n",
    "a= cosine_similarity( sparse_matrix, sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
